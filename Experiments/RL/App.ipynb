{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_single_objects = \"../COTSDataset/Part 1 - Single Objects\"\n",
    "part2_multiple_objects = \"../COTSDataset/Part 2 - Multiple Objects\"\n",
    "part3_complex_background = \"../COTSDataset/Part 3 - Complex Background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(part2_multiple_objects, \"academic_book_no\", \"1_colour.jpeg\")\n",
    "# img_path = os.path.join(part2_multiple_objects, \"academic_book_no\", \"2_colour.jpeg\")\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_path = os.path.join(part2_multiple_objects, \"academic_book_no\", \"masks\", \"ac_3_colour_mask_7_mask.png\")\n",
    "# mask_path = os.path.join(part2_multiple_objects, \"academic_book_no\", \"masks\", \"ac_3_colour_mask_8_mask.png\")\n",
    "mask = cv2.imread(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting target bounding box from mask\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "mask = mask.astype(np.uint8)\n",
    "# Find contours in the mask\n",
    "contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Initialize an empty list to store bounding boxes\n",
    "bounding_boxes = []\n",
    "\n",
    "# Loop through the detected contours\n",
    "for cnt in contours:\n",
    "    # Get the bounding box for each contour\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    \n",
    "    # Add the bounding box coordinates to the list\n",
    "    bounding_boxes.append((x, y, x + w, y + h))\n",
    "\n",
    "# Find the minimum and maximum coordinates to create a bounding box around all masks\n",
    "x1, y1 = min(box[0] for box in bounding_boxes), min(box[1] for box in bounding_boxes)\n",
    "x2, y2 = max(box[2] for box in bounding_boxes), max(box[3] for box in bounding_boxes)\n",
    "\n",
    "# Create the target bounding box that encompasses all masks\n",
    "target_bbox = np.array([x1, y1, x2, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img)\n",
    "plot_img(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n",
    "from env import *\n",
    "from init import *\n",
    "from agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('DetectionEnv-v0', image = img, original_image = img, target_bbox = target_bbox)\n",
    "env.reset()\n",
    "# PROBLEM WITH RENDERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment\n",
    "# env = DetectionEnv(img, img, target_bbox) #, feature_extractor=ResNet50FeatureExtractor(), target_size=RESNET50_TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DetectionEnv(img, img, target_bbox, feature_extractor=ResNet50FeatureExtractor(), target_size=RESNET50_TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = DetectionEnv(img, img, target_bbox, feature_extractor=MobileNetV2FeatureExtractor(), target_size=MOBILENETV2_TARGET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = env.show(do_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = env.show(do_display=True, mode='bbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = env.show(do_display=True, mode='heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space, info = env.reset()\n",
    "print(observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "env.decode_action(action)\n",
    "\n",
    "# Taking a step in the environment\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "print(\"Terminated: \", terminated)\n",
    "print(\"Truncated: \", truncated)\n",
    "print(\"Reward: \", reward)\n",
    "print(\"IoU\", info['iou'])\n",
    "print(\"Recall\", info['recall'])\n",
    "_ = env.show(do_display=True, mode='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path2 = os.path.join(part2_multiple_objects, \"academic_book_no\", \"2_colour.jpeg\")\n",
    "# img_path = os.path.join(part2_multiple_objects, \"academic_book_no\", \"2_colour.jpeg\")\n",
    "img2 = cv2.imread(img_path2)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(image1, image2):\n",
    "    for i in range(len(image1)):\n",
    "        for j in range(len(image1[0])):\n",
    "            if image1[i][j] != image2[i][j]:\n",
    "                print(\"Not equal\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, _ = env.reset(image=img)\n",
    "image2, _ = env.reset(image=img2)\n",
    "check(image1, image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, _ = env.reset(image=img)\n",
    "image2, _ = env.reset(image=img)\n",
    "check(image1, image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "save_path =\"DQN_model1\"\n",
    "agent = DQNAgent(input_size, output_size, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the agent\n",
    "num_episodes = 100000\n",
    "rewards_list = []\n",
    "iou_list = []\n",
    "recall_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    state = env.get_state()\n",
    "    state = state  # Move the state tensor to CUDA if available\n",
    "    total_reward = 0\n",
    "    # agent.reset()\n",
    "    \n",
    "    print(\"Epsilon: \", agent.epsilon)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # print(\"Truncated: \", truncated)\n",
    "        done = truncated or terminated\n",
    "        next_state = torch.Tensor(next_state).view(1, -1)  # Convert the next_state to a PyTorch tensor and move to CUDA if available\n",
    "        total_reward += reward\n",
    "\n",
    "        # Ensure all tensors used in replay buffer are on the CUDA device\n",
    "        agent.replay_buffer.push(Transition(state, action, reward, done, next_state))\n",
    "\n",
    "        state = next_state\n",
    "        agent.update()\n",
    "\n",
    "        counter += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # agent.update_epsilon()\n",
    "\n",
    "    rewards_list.append(total_reward)\n",
    "    iou_list.append(info['iou'])\n",
    "    recall_list.append(info['recall'])\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}, IoU: {info['iou']}, Recall: {info['recall']}, Steps: {env.step_count}, Threshold: {info['threshold']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "\n",
    "# model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "# obs, info = env.reset()\n",
    "\n",
    "# # Train the model for a certain number of time steps\n",
    "# total_timesteps = 10000\n",
    "# update_frequency = 4  # Update the model every 4 steps\n",
    "# for timestep in range(total_timesteps):\n",
    "#     print(f\"Timestep {timestep + 1}/{total_timesteps}\")\n",
    "#     print(obs)\n",
    "#     obs_np = obs.detach().numpy()\n",
    "#     action, _states = model.predict(obs_np, deterministic=True)\n",
    "#     obs, reward, terminated, truncated, info = env.step(action[0])\n",
    "    \n",
    "#     # Incrementally train the model\n",
    "#     if timestep % update_frequency == 0:\n",
    "#         obs_np = obs.detach().numpy()\n",
    "#         model.learn(obs_np, action, reward, obs_np, terminated)  # Update the model using a single step of experience\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#         obs, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, info = env.reset()\n",
    "# while True:\n",
    "#     obs = obs.detach().numpy()\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     print(action[0])\n",
    "#     obs, reward, terminated, truncated, info = env.step(action[0])\n",
    "#     if terminated or truncated:\n",
    "#         obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the rewards\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(rewards_list, label='Total Reward', color='blue')\n",
    "plt.plot(iou_list, label='IoU', color='orange')\n",
    "plt.plot(recall_list, label='Recall', color='purple')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward vs Episode')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the bounding box\n",
    "env.reset()\n",
    "state = env.get_state()\n",
    "state = state.to(device)  # Move the state tensor to CUDA if available\n",
    "total_reward = 0\n",
    "agent.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while True:\n",
    "    action = agent.select_action(state)\n",
    "    next_state, _, terminated, truncated, info = env.step(action)\n",
    "    env.decode_action(action)\n",
    "    done = truncated or terminated\n",
    "    next_state = torch.Tensor(next_state).view(1, -1).to(device)  # Convert the next_state to a PyTorch tensor and move to CUDA if available\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    env.show(do_display=True, mode='image')\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Total Reward: {total_reward}, IoU: {info['iou']}, Recall: {info['recall']}, Steps: {env.step_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
